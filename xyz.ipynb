{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2cfce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import torch; print(torch.cuda.is_available())\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e4c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch scikit-learn pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66bff04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import numpy as np\n",
    "import datasets.formatting.formatting\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# ‚úÖ Monkey patch for NumPy 2.0 error in datasets\n",
    "def fixed_arrow_array_to_numpy(self, pa_array):\n",
    "    array = pa_array.to_pandas().values\n",
    "    return np.array(array, copy=True)\n",
    "\n",
    "datasets.formatting.formatting.NumpyArrowExtractor._arrow_array_to_numpy = fixed_arrow_array_to_numpy\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# ‚úÖ Disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ‚úÖ Load dataframe with better preprocessing\n",
    "df = pd.read_excel(r\"BanglaBlendCleanedDataWithEnglishTranslation.xlsx\")\n",
    "df = df.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd3c5d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentence(English Translation)</th>\n",
       "      <th>Labels(English)</th>\n",
       "      <th>Labels(Bangla)</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡¶ï‡¶æ‡¶∞ ‡¶ú‡¶æ‡¶®‡¶æ‡¶≤‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶∏‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶á‡¶§‡ßá‡¶õ‡¶ø‡¶≤</td>\n",
       "      <td>The sea could be seen from the window there</td>\n",
       "      <td>Saint</td>\n",
       "      <td>Sadhu(‡¶∏‡¶æ‡¶ß‡ßÅ )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¶‡ßá‡¶ñ‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø‡¶§‡ßá‡¶õ‡¶ø ‡¶®‡¶æ</td>\n",
       "      <td>I can't see anything</td>\n",
       "      <td>Saint</td>\n",
       "      <td>Sadhu(‡¶∏‡¶æ‡¶ß‡ßÅ )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡¶∏‡¶ï‡¶≤‡ßá‡¶∞‡¶á ‡¶Ö‡¶®‡¶æ‡¶¨‡ßÉ‡¶§ ‡¶¶‡ßá‡¶π ‡¶∏‡¶ï‡¶≤‡ßá‡¶∞ ‡¶∏‡ßá‡¶á ‡¶Ö‡¶®‡¶æ‡¶¨‡ßÉ‡¶§ ‡¶¨‡¶ï‡ßç‡¶∑‡ßá ‡¶Ü‡¶∞‡¶∂‡¶ø‡¶∞...</td>\n",
       "      <td>Everyone's naked body is burning once in the m...</td>\n",
       "      <td>Saint</td>\n",
       "      <td>Sadhu(‡¶∏‡¶æ‡¶ß‡ßÅ )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡¶Æ‡ßá‡ßü‡ßá‡¶ü‡¶ø ‡¶∏‡ßá‡¶¶‡¶ø‡¶® ‡¶≠‡¶ø‡¶ï‡ßç‡¶∑‡ßÅ‡¶ï‡¶ï‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡¶ø‡ßü‡¶æ‡¶õ‡¶ø‡¶≤</td>\n",
       "      <td>The girl had helped the beggar that day</td>\n",
       "      <td>Saint</td>\n",
       "      <td>Sadhu(‡¶∏‡¶æ‡¶ß‡ßÅ )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶™‡ßç‡¶∞‡¶∂‡¶Ç‡¶∏‡¶æ ‡¶ï‡¶∞ ‡¶®‡¶æ ‡¶ï‡¶∞ ‡¶¨‡ßÉ‡¶¶‡ßç‡¶ß ‡¶¨‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ ‡¶§‡ßã‡¶Æ‡¶æ‡¶Ø‡¶º ‡¶™‡ßÅ‡¶∞‡¶æ...</td>\n",
       "      <td>You don't praise the old man and he will tell ...</td>\n",
       "      <td>Saint</td>\n",
       "      <td>Sadhu(‡¶∏‡¶æ‡¶ß‡ßÅ )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7345</th>\n",
       "      <td>‡¶∞‡¶®‡ßç‡¶ß‡¶®‡¶™‡ßç‡¶∞‡¶£‡¶æ‡¶≤‡ßÄ ‡¶¶‡ßã‡¶™‡ßá‡¶Ø‡¶º‡¶æ‡¶ú‡¶æ ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º</td>\n",
       "      <td>The cuisine Dopeyazza is popular</td>\n",
       "      <td>Common</td>\n",
       "      <td>Cholito(‡¶ö‡¶≤‡¶ø‡¶§)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7346</th>\n",
       "      <td>‡¶∂‡ßá‡¶∑‡ßá ‡¶∞‡¶π‡¶ø‡¶Æ ‡¶ï‡¶∞‡¶ø‡¶Æ‡¶ï‡ßá ‡¶¨‡¶ø‡¶™‡¶¶‡ßá ‡¶´‡ßá‡¶≤‡¶≤</td>\n",
       "      <td>At the end, Rahim put Karim in danger</td>\n",
       "      <td>Common</td>\n",
       "      <td>Cholito(‡¶ö‡¶≤‡¶ø‡¶§)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>‡¶π‡¶æ‡¶´‡¶ø‡¶ú ‡¶ï‡ßá ‡¶§‡¶æ‡¶∞‡¶æ‡¶á ‡¶¨‡¶ø‡¶™‡¶¶‡ßá ‡¶´‡ßá‡¶≤‡¶≤</td>\n",
       "      <td>They put Hafiz in danger</td>\n",
       "      <td>Common</td>\n",
       "      <td>Cholito(‡¶ö‡¶≤‡¶ø‡¶§)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>‡¶∏‡¶æ‡¶≤‡ßá ‡¶Ü‡¶≤‡ßá‡¶ï‡¶ú‡¶æ‡¶®‡ßç‡¶°‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶®‡¶ø‡¶Ç‡¶π‡¶æ‡¶Æ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶π‡¶°‡¶º‡¶™‡ßç‡¶™‡¶æ ‡¶∏‡¶ø‡¶≤‡¶Æ‡ßã‡¶π...</td>\n",
       "      <td>Alexander Cunningham published the first Harap...</td>\n",
       "      <td>Common</td>\n",
       "      <td>Cholito(‡¶ö‡¶≤‡¶ø‡¶§)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>‡¶∏‡ßÅ‡¶Æ‡¶® ‡¶≠‡¶æ‡¶§ ‡¶ñ‡ßá‡¶Ø‡¶º‡ßá ‡¶ñ‡ßá‡¶≤‡¶§‡ßá ‡¶Ø‡¶æ‡¶¨‡ßá</td>\n",
       "      <td>Suman will eat rice and go to play</td>\n",
       "      <td>Common</td>\n",
       "      <td>Cholito(‡¶ö‡¶≤‡¶ø‡¶§)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7350 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  \\\n",
       "0             ‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡¶ï‡¶æ‡¶∞ ‡¶ú‡¶æ‡¶®‡¶æ‡¶≤‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶∏‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶á‡¶§‡ßá‡¶õ‡¶ø‡¶≤   \n",
       "1                           ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¶‡ßá‡¶ñ‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø‡¶§‡ßá‡¶õ‡¶ø ‡¶®‡¶æ   \n",
       "2     ‡¶∏‡¶ï‡¶≤‡ßá‡¶∞‡¶á ‡¶Ö‡¶®‡¶æ‡¶¨‡ßÉ‡¶§ ‡¶¶‡ßá‡¶π ‡¶∏‡¶ï‡¶≤‡ßá‡¶∞ ‡¶∏‡ßá‡¶á ‡¶Ö‡¶®‡¶æ‡¶¨‡ßÉ‡¶§ ‡¶¨‡¶ï‡ßç‡¶∑‡ßá ‡¶Ü‡¶∞‡¶∂‡¶ø‡¶∞...   \n",
       "3               ‡¶Æ‡ßá‡ßü‡ßá‡¶ü‡¶ø ‡¶∏‡ßá‡¶¶‡¶ø‡¶® ‡¶≠‡¶ø‡¶ï‡ßç‡¶∑‡ßÅ‡¶ï‡¶ï‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡¶ø‡ßü‡¶æ‡¶õ‡¶ø‡¶≤   \n",
       "4     ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶™‡ßç‡¶∞‡¶∂‡¶Ç‡¶∏‡¶æ ‡¶ï‡¶∞ ‡¶®‡¶æ ‡¶ï‡¶∞ ‡¶¨‡ßÉ‡¶¶‡ßç‡¶ß ‡¶¨‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ ‡¶§‡ßã‡¶Æ‡¶æ‡¶Ø‡¶º ‡¶™‡ßÅ‡¶∞‡¶æ...   \n",
       "...                                                 ...   \n",
       "7345                    ‡¶∞‡¶®‡ßç‡¶ß‡¶®‡¶™‡ßç‡¶∞‡¶£‡¶æ‡¶≤‡ßÄ ‡¶¶‡ßã‡¶™‡ßá‡¶Ø‡¶º‡¶æ‡¶ú‡¶æ ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º   \n",
       "7346                        ‡¶∂‡ßá‡¶∑‡ßá ‡¶∞‡¶π‡¶ø‡¶Æ ‡¶ï‡¶∞‡¶ø‡¶Æ‡¶ï‡ßá ‡¶¨‡¶ø‡¶™‡¶¶‡ßá ‡¶´‡ßá‡¶≤‡¶≤   \n",
       "7347                          ‡¶π‡¶æ‡¶´‡¶ø‡¶ú ‡¶ï‡ßá ‡¶§‡¶æ‡¶∞‡¶æ‡¶á ‡¶¨‡¶ø‡¶™‡¶¶‡ßá ‡¶´‡ßá‡¶≤‡¶≤   \n",
       "7348  ‡¶∏‡¶æ‡¶≤‡ßá ‡¶Ü‡¶≤‡ßá‡¶ï‡¶ú‡¶æ‡¶®‡ßç‡¶°‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶®‡¶ø‡¶Ç‡¶π‡¶æ‡¶Æ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶π‡¶°‡¶º‡¶™‡ßç‡¶™‡¶æ ‡¶∏‡¶ø‡¶≤‡¶Æ‡ßã‡¶π...   \n",
       "7349                          ‡¶∏‡ßÅ‡¶Æ‡¶® ‡¶≠‡¶æ‡¶§ ‡¶ñ‡ßá‡¶Ø‡¶º‡ßá ‡¶ñ‡ßá‡¶≤‡¶§‡ßá ‡¶Ø‡¶æ‡¶¨‡ßá   \n",
       "\n",
       "                          Sentence(English Translation) Labels(English)  \\\n",
       "0           The sea could be seen from the window there           Saint   \n",
       "1                                  I can't see anything           Saint   \n",
       "2     Everyone's naked body is burning once in the m...           Saint   \n",
       "3               The girl had helped the beggar that day           Saint   \n",
       "4     You don't praise the old man and he will tell ...           Saint   \n",
       "...                                                 ...             ...   \n",
       "7345                   The cuisine Dopeyazza is popular          Common   \n",
       "7346              At the end, Rahim put Karim in danger          Common   \n",
       "7347                           They put Hafiz in danger          Common   \n",
       "7348  Alexander Cunningham published the first Harap...          Common   \n",
       "7349                 Suman will eat rice and go to play          Common   \n",
       "\n",
       "     Labels(Bangla)  Labels  \n",
       "0      Sadhu(‡¶∏‡¶æ‡¶ß‡ßÅ )       0  \n",
       "1      Sadhu(‡¶∏‡¶æ‡¶ß‡ßÅ )       0  \n",
       "2      Sadhu(‡¶∏‡¶æ‡¶ß‡ßÅ )       0  \n",
       "3      Sadhu(‡¶∏‡¶æ‡¶ß‡ßÅ )       0  \n",
       "4      Sadhu(‡¶∏‡¶æ‡¶ß‡ßÅ )       0  \n",
       "...             ...     ...  \n",
       "7345  Cholito(‡¶ö‡¶≤‡¶ø‡¶§)       1  \n",
       "7346  Cholito(‡¶ö‡¶≤‡¶ø‡¶§)       1  \n",
       "7347  Cholito(‡¶ö‡¶≤‡¶ø‡¶§)       1  \n",
       "7348  Cholito(‡¶ö‡¶≤‡¶ø‡¶§)       1  \n",
       "7349  Cholito(‡¶ö‡¶≤‡¶ø‡¶§)       1  \n",
       "\n",
       "[7350 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "134c441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Keep only needed columns and rename\n",
    "df = df[[\"Sentence\", \"Labels\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75e606c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after clean & rename: Index(['text', 'label'], dtype='object')\n",
      "Data shape: (7066, 2)\n",
      "Label distribution:\n",
      "label\n",
      "1    3608\n",
      "0    3458\n",
      "Name: count, dtype: int64\n",
      "                                                text  label\n",
      "0          ‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡¶ï‡¶æ‡¶∞ ‡¶ú‡¶æ‡¶®‡¶æ‡¶≤‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶∏‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶á‡¶§‡ßá‡¶õ‡¶ø‡¶≤      0\n",
      "1                        ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¶‡ßá‡¶ñ‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø‡¶§‡ßá‡¶õ‡¶ø ‡¶®‡¶æ      0\n",
      "2  ‡¶∏‡¶ï‡¶≤‡ßá‡¶∞‡¶á ‡¶Ö‡¶®‡¶æ‡¶¨‡ßÉ‡¶§ ‡¶¶‡ßá‡¶π ‡¶∏‡¶ï‡¶≤‡ßá‡¶∞ ‡¶∏‡ßá‡¶á ‡¶Ö‡¶®‡¶æ‡¶¨‡ßÉ‡¶§ ‡¶¨‡¶ï‡ßç‡¶∑‡ßá ‡¶Ü‡¶∞‡¶∂‡¶ø‡¶∞...      0\n",
      "3            ‡¶Æ‡ßá‡ßü‡ßá‡¶ü‡¶ø ‡¶∏‡ßá‡¶¶‡¶ø‡¶® ‡¶≠‡¶ø‡¶ï‡ßç‡¶∑‡ßÅ‡¶ï‡¶ï‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡¶ø‡ßü‡¶æ‡¶õ‡¶ø‡¶≤      0\n",
      "4  ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶™‡ßç‡¶∞‡¶∂‡¶Ç‡¶∏‡¶æ ‡¶ï‡¶∞ ‡¶®‡¶æ ‡¶ï‡¶∞ ‡¶¨‡ßÉ‡¶¶‡ßç‡¶ß ‡¶¨‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ ‡¶§‡ßã‡¶Æ‡¶æ‡¶Ø‡¶º ‡¶™‡ßÅ‡¶∞‡¶æ...      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pust\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77360c2bb46f43f699363bbe419920b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4946\n",
      "Validation size: 1060\n",
      "Test size: 1060\n",
      "üöÄ Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ae590e7c364baa895ff5cc00cdb81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7189, 'grad_norm': 2.8138017654418945, 'learning_rate': 3e-06, 'epoch': 0.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd6806e1a704aee907ae26abf1a0948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4733998477458954, 'eval_accuracy': 0.8509433962264151, 'eval_f1': 0.8508982454257666, 'eval_precision': 0.8510047605821217, 'eval_recall': 0.8509433962264151, 'eval_runtime': 8.1247, 'eval_samples_per_second': 130.467, 'eval_steps_per_second': 4.185, 'epoch': 0.99}\n",
      "{'loss': 0.535, 'grad_norm': 3.4255077838897705, 'learning_rate': 6e-06, 'epoch': 1.29}\n",
      "{'loss': 0.4081, 'grad_norm': 7.16697359085083, 'learning_rate': 9e-06, 'epoch': 1.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78d32ba720940898ee0b7422d4c31d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3474562168121338, 'eval_accuracy': 0.9122641509433962, 'eval_f1': 0.9122578219850621, 'eval_precision': 0.9122661172275058, 'eval_recall': 0.9122641509433962, 'eval_runtime': 7.8355, 'eval_samples_per_second': 135.282, 'eval_steps_per_second': 4.339, 'epoch': 2.0}\n",
      "{'loss': 0.3602, 'grad_norm': 4.683022975921631, 'learning_rate': 1.2e-05, 'epoch': 2.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9d17e805d94eb8bf5120d704f910f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32172346115112305, 'eval_accuracy': 0.9245283018867925, 'eval_f1': 0.9244075971760487, 'eval_precision': 0.9259827399112566, 'eval_recall': 0.9245283018867925, 'eval_runtime': 7.9948, 'eval_samples_per_second': 132.587, 'eval_steps_per_second': 4.253, 'epoch': 2.99}\n",
      "{'loss': 0.3418, 'grad_norm': 4.497650623321533, 'learning_rate': 1.5e-05, 'epoch': 3.23}\n",
      "{'loss': 0.3177, 'grad_norm': 2.5612897872924805, 'learning_rate': 1.8e-05, 'epoch': 3.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ae832061b441d2b5cbc9d2b677ca26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31336912512779236, 'eval_accuracy': 0.939622641509434, 'eval_f1': 0.9395994009089657, 'eval_precision': 0.9398174565119123, 'eval_recall': 0.939622641509434, 'eval_runtime': 7.9076, 'eval_samples_per_second': 134.048, 'eval_steps_per_second': 4.3, 'epoch': 4.0}\n",
      "{'loss': 0.3062, 'grad_norm': 5.7067484855651855, 'learning_rate': 2.1e-05, 'epoch': 4.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb00328db6f84989b2d764fab651282b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31750261783599854, 'eval_accuracy': 0.9367924528301886, 'eval_f1': 0.9367315219341047, 'eval_precision': 0.9375562605517523, 'eval_recall': 0.9367924528301886, 'eval_runtime': 7.904, 'eval_samples_per_second': 134.109, 'eval_steps_per_second': 4.302, 'epoch': 4.99}\n",
      "{'loss': 0.2874, 'grad_norm': 6.30911111831665, 'learning_rate': 2.4e-05, 'epoch': 5.16}\n",
      "{'loss': 0.2858, 'grad_norm': 4.2460222244262695, 'learning_rate': 2.7000000000000002e-05, 'epoch': 5.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db78574e17da425e95d9a9de565cc694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31065577268600464, 'eval_accuracy': 0.940566037735849, 'eval_f1': 0.9405011013563359, 'eval_precision': 0.9414873079524106, 'eval_recall': 0.940566037735849, 'eval_runtime': 7.9772, 'eval_samples_per_second': 132.879, 'eval_steps_per_second': 4.262, 'epoch': 6.0}\n",
      "{'loss': 0.2757, 'grad_norm': 6.265702724456787, 'learning_rate': 3e-05, 'epoch': 6.45}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f191e94446d4be480e96d72e43375da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3196853995323181, 'eval_accuracy': 0.9367924528301886, 'eval_f1': 0.9367937472929474, 'eval_precision': 0.9367967098544635, 'eval_recall': 0.9367924528301886, 'eval_runtime': 7.8186, 'eval_samples_per_second': 135.574, 'eval_steps_per_second': 4.349, 'epoch': 6.99}\n",
      "{'loss': 0.2595, 'grad_norm': 5.290413856506348, 'learning_rate': 2.9908960159769243e-05, 'epoch': 7.1}\n",
      "{'loss': 0.2471, 'grad_norm': 11.927033424377441, 'learning_rate': 2.9636945739411533e-05, 'epoch': 7.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159560caa44b48aaa140144608f7ac78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31967398524284363, 'eval_accuracy': 0.9433962264150944, 'eval_f1': 0.9433056978820366, 'eval_precision': 0.944945895500684, 'eval_recall': 0.9433962264150944, 'eval_runtime': 8.3189, 'eval_samples_per_second': 127.42, 'eval_steps_per_second': 4.087, 'epoch': 8.0}\n",
      "{'loss': 0.2522, 'grad_norm': 2.297903060913086, 'learning_rate': 2.9187258625509518e-05, 'epoch': 8.39}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1c81b564fe420194886b164c42582b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3327191174030304, 'eval_accuracy': 0.9349056603773584, 'eval_f1': 0.9349042120328063, 'eval_precision': 0.9359434664608428, 'eval_recall': 0.9349056603773584, 'eval_runtime': 7.7617, 'eval_samples_per_second': 136.569, 'eval_steps_per_second': 4.381, 'epoch': 8.99}\n",
      "{'loss': 0.2625, 'grad_norm': 2.0402607917785645, 'learning_rate': 2.8565357410463664e-05, 'epoch': 9.03}\n",
      "{'loss': 0.2366, 'grad_norm': 13.379804611206055, 'learning_rate': 2.7778791132574908e-05, 'epoch': 9.68}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a686bb3eb74f13901cc33b58295290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3165685832500458, 'eval_accuracy': 0.9433962264150944, 'eval_f1': 0.9433744383521554, 'eval_precision': 0.9435949183330448, 'eval_recall': 0.9433962264150944, 'eval_runtime': 7.959, 'eval_samples_per_second': 133.183, 'eval_steps_per_second': 4.272, 'epoch': 10.0}\n",
      "{'loss': 0.243, 'grad_norm': 10.441143989562988, 'learning_rate': 2.6837107640945904e-05, 'epoch': 10.32}\n",
      "{'loss': 0.2278, 'grad_norm': 5.561850547790527, 'learning_rate': 2.575173769752677e-05, 'epoch': 10.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe858343702411b9643498d36217785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3327750861644745, 'eval_accuracy': 0.9415094339622642, 'eval_f1': 0.9415000562995878, 'eval_precision': 0.9415510733714365, 'eval_recall': 0.9415094339622642, 'eval_runtime': 7.7384, 'eval_samples_per_second': 136.978, 'eval_steps_per_second': 4.394, 'epoch': 10.99}\n",
      "{'loss': 0.2212, 'grad_norm': 4.853788375854492, 'learning_rate': 2.4535856223149525e-05, 'epoch': 11.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638ea131e9634964b48f995c58a431b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3402288854122162, 'eval_accuracy': 0.940566037735849, 'eval_f1': 0.9404356115103488, 'eval_precision': 0.9428739362516778, 'eval_recall': 0.940566037735849, 'eval_runtime': 7.6409, 'eval_samples_per_second': 138.728, 'eval_steps_per_second': 4.45, 'epoch': 12.0}\n",
      "{'loss': 0.2293, 'grad_norm': 0.765144407749176, 'learning_rate': 2.3204222371836406e-05, 'epoch': 12.26}\n",
      "{'loss': 0.224, 'grad_norm': 0.20011042058467865, 'learning_rate': 2.177300037466334e-05, 'epoch': 12.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af96ac444ee64f43ac7cb123fdb70542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3445494472980499, 'eval_accuracy': 0.9386792452830188, 'eval_f1': 0.9386673923153476, 'eval_precision': 0.9387375057730849, 'eval_recall': 0.9386792452830188, 'eval_runtime': 7.5247, 'eval_samples_per_second': 140.869, 'eval_steps_per_second': 4.518, 'epoch': 12.99}\n",
      "{'loss': 0.2183, 'grad_norm': 0.73643958568573, 'learning_rate': 2.025956332789132e-05, 'epoch': 13.55}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a59379ff9241dcb2698080ccc820b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3305486738681793, 'eval_accuracy': 0.9443396226415094, 'eval_f1': 0.9443455191251242, 'eval_precision': 0.9444337859284077, 'eval_recall': 0.9443396226415094, 'eval_runtime': 7.5164, 'eval_samples_per_second': 141.026, 'eval_steps_per_second': 4.523, 'epoch': 14.0}\n",
      "{'loss': 0.2193, 'grad_norm': 0.3022664189338684, 'learning_rate': 1.8682282307111988e-05, 'epoch': 14.19}\n",
      "{'loss': 0.2135, 'grad_norm': 0.21690213680267334, 'learning_rate': 1.7060303367276123e-05, 'epoch': 14.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db5fa00dd364da4a9cbdf65e61768da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3211290240287781, 'eval_accuracy': 0.9415094339622642, 'eval_f1': 0.9415094339622642, 'eval_precision': 0.9424752360002655, 'eval_recall': 0.9415094339622642, 'eval_runtime': 7.6176, 'eval_samples_per_second': 139.151, 'eval_steps_per_second': 4.463, 'epoch': 14.99}\n",
      "{'loss': 0.22, 'grad_norm': 1.0165290832519531, 'learning_rate': 1.5413315135522434e-05, 'epoch': 15.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ad2ec3cc89414580a9f89d9209f7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32899004220962524, 'eval_accuracy': 0.9415094339622642, 'eval_f1': 0.9415000562995878, 'eval_precision': 0.9415510733714365, 'eval_recall': 0.9415094339622642, 'eval_runtime': 7.5378, 'eval_samples_per_second': 140.625, 'eval_steps_per_second': 4.511, 'epoch': 16.0}\n",
      "{'loss': 0.2097, 'grad_norm': 2.428420305252075, 'learning_rate': 1.3761309817915017e-05, 'epoch': 16.13}\n",
      "{'loss': 0.2082, 'grad_norm': 0.06486884504556656, 'learning_rate': 1.2124340521143929e-05, 'epoch': 16.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905a4d13352d4468b6bc949ee77f74f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3280755877494812, 'eval_accuracy': 0.9471698113207547, 'eval_f1': 0.9471283634664617, 'eval_precision': 0.9477636052839573, 'eval_recall': 0.9471698113207547, 'eval_runtime': 7.6149, 'eval_samples_per_second': 139.201, 'eval_steps_per_second': 4.465, 'epoch': 16.99}\n",
      "{'loss': 0.2107, 'grad_norm': 0.12779253721237183, 'learning_rate': 1.0522277834974586e-05, 'epoch': 17.42}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4668d47b66b43b585f5a516a4e9bae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3247220516204834, 'eval_accuracy': 0.9490566037735849, 'eval_f1': 0.9490449862889477, 'eval_precision': 0.9491416271482864, 'eval_recall': 0.9490566037735849, 'eval_runtime': 7.6191, 'eval_samples_per_second': 139.123, 'eval_steps_per_second': 4.462, 'epoch': 18.0}\n",
      "{'loss': 0.209, 'grad_norm': 0.24474157392978668, 'learning_rate': 8.974568630205462e-06, 'epoch': 18.06}\n",
      "{'loss': 0.2058, 'grad_norm': 0.6760526895523071, 'learning_rate': 7.500000000000004e-06, 'epoch': 18.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc21e35195e14619a014ce2767b0ecfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33308282494544983, 'eval_accuracy': 0.9452830188679245, 'eval_f1': 0.9452705408288697, 'eval_precision': 0.9453658897521272, 'eval_recall': 0.9452830188679245, 'eval_runtime': 7.7781, 'eval_samples_per_second': 136.28, 'eval_steps_per_second': 4.371, 'epoch': 18.99}\n",
      "{'loss': 0.2053, 'grad_norm': 0.13216090202331543, 'learning_rate': 6.116471210025302e-06, 'epoch': 19.35}\n",
      "{'loss': 0.2049, 'grad_norm': 0.05667274072766304, 'learning_rate': 4.840776425613887e-06, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63798e6367794f92a657b2ec7beafe7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33405038714408875, 'eval_accuracy': 0.9471698113207547, 'eval_f1': 0.947153808505335, 'eval_precision': 0.947306669982495, 'eval_recall': 0.9471698113207547, 'eval_runtime': 7.5536, 'eval_samples_per_second': 140.33, 'eval_steps_per_second': 4.501, 'epoch': 20.0}\n",
      "{'loss': 0.2043, 'grad_norm': 0.8620634078979492, 'learning_rate': 3.688400853346558e-06, 'epoch': 20.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d91c885175647d4b7c717ed6113f2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3362462520599365, 'eval_accuracy': 0.9433962264150944, 'eval_f1': 0.9433639390576126, 'eval_precision': 0.9437626514298524, 'eval_recall': 0.9433962264150944, 'eval_runtime': 7.714, 'eval_samples_per_second': 137.413, 'eval_steps_per_second': 4.408, 'epoch': 20.99}\n",
      "{'loss': 0.205, 'grad_norm': 0.0326048843562603, 'learning_rate': 2.673332771621324e-06, 'epoch': 21.29}\n",
      "{'loss': 0.2047, 'grad_norm': 0.06307166814804077, 'learning_rate': 1.8078937319026655e-06, 'epoch': 21.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c1d663fe8e4f0083989cdc2d5e0f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3365718126296997, 'eval_accuracy': 0.9443396226415094, 'eval_f1': 0.9443288637939308, 'eval_precision': 0.9444005666450573, 'eval_recall': 0.9443396226415094, 'eval_runtime': 7.4782, 'eval_samples_per_second': 141.745, 'eval_steps_per_second': 4.547, 'epoch': 22.0}\n",
      "{'loss': 0.205, 'grad_norm': 4.0086565017700195, 'learning_rate': 1.1025889917779735e-06, 'epoch': 22.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9c82f7922b4b40b6491ffd0daa0dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33672428131103516, 'eval_accuracy': 0.9415094339622642, 'eval_f1': 0.941496095368792, 'eval_precision': 0.941590152355968, 'eval_recall': 0.9415094339622642, 'eval_runtime': 7.6185, 'eval_samples_per_second': 139.135, 'eval_steps_per_second': 4.463, 'epoch': 22.99}\n",
      "{'loss': 0.2043, 'grad_norm': 0.0563100203871727, 'learning_rate': 5.659799953612438e-07, 'epoch': 23.23}\n",
      "{'loss': 0.2047, 'grad_norm': 0.06765048950910568, 'learning_rate': 2.0458044895916516e-07, 'epoch': 23.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ebb50c8a4b4dd1bfc9f37dad916303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3357042670249939, 'eval_accuracy': 0.9443396226415094, 'eval_f1': 0.9443157660188449, 'eval_precision': 0.9445765698446439, 'eval_recall': 0.9443396226415094, 'eval_runtime': 7.4954, 'eval_samples_per_second': 141.419, 'eval_steps_per_second': 4.536, 'epoch': 24.0}\n",
      "{'loss': 0.2031, 'grad_norm': 1.752881646156311, 'learning_rate': 2.2777253500257388e-08, 'epoch': 24.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f5c167249f4971afca189086e4201e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33562329411506653, 'eval_accuracy': 0.9452830188679245, 'eval_f1': 0.9452570782376002, 'eval_precision': 0.9455616638764459, 'eval_recall': 0.9452830188679245, 'eval_runtime': 7.8514, 'eval_samples_per_second': 135.008, 'eval_steps_per_second': 4.33, 'epoch': 24.84}\n",
      "{'train_runtime': 582.2508, 'train_samples_per_second': 212.366, 'train_steps_per_second': 3.306, 'train_loss': 0.2622764894559786, 'epoch': 24.84}\n",
      "\n",
      "üìä Validation Results:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a70caa95253496e9801a20f863ef9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 0.3247\n",
      "eval_accuracy: 0.9491\n",
      "eval_f1: 0.9490\n",
      "eval_precision: 0.9491\n",
      "eval_recall: 0.9491\n",
      "eval_runtime: 7.4967\n",
      "eval_samples_per_second: 141.3960\n",
      "eval_steps_per_second: 4.5350\n",
      "epoch: 24.8387\n",
      "\n",
      "üìä Test Results:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72da1d686a5649009de426e7868570f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_eval_loss: 0.3210\n",
      "test_eval_accuracy: 0.9509\n",
      "test_eval_f1: 0.9509\n",
      "test_eval_precision: 0.9511\n",
      "test_eval_recall: 0.9509\n",
      "test_eval_runtime: 7.6044\n",
      "test_eval_samples_per_second: 139.3940\n",
      "test_eval_steps_per_second: 4.4710\n",
      "test_epoch: 24.8387\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc537b985171413aa0b101e40076a454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Overfitting Analysis:\n",
      "Training Accuracy: 0.9992\n",
      "Test Accuracy: 0.9509\n",
      "Accuracy Gap (overfitting): 0.0482\n",
      "F1 Gap (overfitting): 0.0483\n",
      "\n",
      "‚úÖ Best model saved at ./final_bangla_bert_model\n",
      "\n",
      "Sample prediction:\n",
      "Text: ‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡•§\n",
      "Predicted class: 1\n",
      "Probabilities: [[0.04099333 0.95900667]]\n"
     ]
    }
   ],
   "source": [
    "df = df.rename(columns={\"Sentence\": \"text\", \"Labels\": \"label\"})\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# ‚úÖ Basic text preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# ‚úÖ Remove empty texts and duplicates\n",
    "df = df[df[\"text\"].str.len() > 0]\n",
    "df = df.drop_duplicates(subset=[\"text\"])\n",
    "\n",
    "print(\"Columns after clean & rename:\", df.columns)\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"Label distribution:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df.head())\n",
    "\n",
    "# ‚úÖ Load tokenizer and model with built-in regularization\n",
    "model_name = \"sagorsarker/bangla-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use standard model with built-in dropout configuration\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,  # Increase dropout for regularization\n",
    "    attention_probs_dropout_prob=0.3,  # Attention dropout\n",
    "    classifier_dropout=0.3  # Classifier dropout\n",
    ").to(device)\n",
    "\n",
    "# ‚úÖ Advanced tokenization with dynamic padding\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"], \n",
    "        padding=False,  # Dynamic padding in data collator\n",
    "        truncation=True, \n",
    "        max_length=256,  # Increased max length for better context\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# ‚úÖ Create dataset with stratified split\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# ‚úÖ Stratified train-validation-test split\n",
    "def create_stratified_splits(dataset, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    labels = [item['label'] for item in dataset]\n",
    "    \n",
    "    # First split: train and temp (val + test)\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=(val_size + test_size), random_state=random_state)\n",
    "    train_idx, temp_idx = next(sss1.split(range(len(labels)), labels))\n",
    "    \n",
    "    # Second split: val and test from temp\n",
    "    temp_labels = [labels[i] for i in temp_idx]\n",
    "    val_ratio = val_size / (val_size + test_size)\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=(1 - val_ratio), random_state=random_state)\n",
    "    val_idx_temp, test_idx_temp = next(sss2.split(range(len(temp_idx)), temp_labels))\n",
    "    \n",
    "    val_idx = [temp_idx[i] for i in val_idx_temp]\n",
    "    test_idx = [temp_idx[i] for i in test_idx_temp]\n",
    "    \n",
    "    return {\n",
    "        'train': dataset.select(train_idx),\n",
    "        'validation': dataset.select(val_idx),\n",
    "        'test': dataset.select(test_idx)\n",
    "    }\n",
    "\n",
    "dataset_splits = create_stratified_splits(dataset)\n",
    "\n",
    "# ‚úÖ Set torch format\n",
    "for split in dataset_splits:\n",
    "    dataset_splits[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "print(f\"Train size: {len(dataset_splits['train'])}\")\n",
    "print(f\"Validation size: {len(dataset_splits['validation'])}\")\n",
    "print(f\"Test size: {len(dataset_splits['test'])}\")\n",
    "\n",
    "# ‚úÖ Data collator for dynamic padding\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ‚úÖ Enhanced metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    recall = recall_score(labels, preds, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ Enhanced training arguments with regularization\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=25,  # Reduced epochs to prevent overfitting\n",
    "    per_device_train_batch_size=32,  # Increased batch size\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,  # Effective batch size = 32\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # ‚úÖ Learning rate scheduling\n",
    "    learning_rate=3e-5,  # Slightly higher learning rate\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=500,\n",
    "    \n",
    "    # ‚úÖ Regularization techniques\n",
    "    weight_decay=0.01,  # L2 regularization\n",
    "    adam_epsilon=1e-6,\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    label_smoothing_factor=0.1,  # Built-in label smoothing\n",
    "    \n",
    "    # ‚úÖ Early stopping and model selection\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # ‚úÖ Evaluation and saving\n",
    "    eval_steps=100,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # ‚úÖ Other optimizations\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# ‚úÖ Standard trainer with built-in label smoothing\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_splits[\"train\"],\n",
    "    eval_dataset=dataset_splits[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# ‚úÖ Train with mixed precision for efficiency\n",
    "print(\"üöÄ Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# ‚úÖ Evaluate on validation set\n",
    "print(\"\\nüìä Validation Results:\")\n",
    "val_metrics = trainer.evaluate()\n",
    "for key, value in val_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# ‚úÖ Evaluate on test set\n",
    "print(\"\\nüìä Test Results:\")\n",
    "test_metrics = trainer.evaluate(dataset_splits[\"test\"])\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"test_{key}: {value:.4f}\")\n",
    "\n",
    "# ‚úÖ Calculate overfitting metrics\n",
    "train_metrics = trainer.evaluate(dataset_splits[\"train\"])\n",
    "overfitting_accuracy = train_metrics['eval_accuracy'] - test_metrics['eval_accuracy']\n",
    "overfitting_f1 = train_metrics['eval_f1'] - test_metrics['eval_f1']\n",
    "\n",
    "print(f\"\\nüéØ Overfitting Analysis:\")\n",
    "print(f\"Training Accuracy: {train_metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"Accuracy Gap (overfitting): {overfitting_accuracy:.4f}\")\n",
    "print(f\"F1 Gap (overfitting): {overfitting_f1:.4f}\")\n",
    "\n",
    "# ‚úÖ Save the best model\n",
    "model.save_pretrained(\"./final_bangla_bert_model\")\n",
    "tokenizer.save_pretrained(\"./final_bangla_bert_model\")\n",
    "print(\"\\n‚úÖ Best model saved at ./final_bangla_bert_model\")\n",
    "\n",
    "# ‚úÖ Optional: Model inference example\n",
    "def predict_text(text, model, tokenizer, device):\n",
    "    \"\"\"Function to predict on new text\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=256\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1)\n",
    "    \n",
    "    return predicted_class.item(), predictions.cpu().numpy()\n",
    "\n",
    "# ‚úÖ Example usage\n",
    "sample_text = \"‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡•§\"\n",
    "try:\n",
    "    pred_class, pred_probs = predict_text(sample_text, model, tokenizer, device)\n",
    "    print(f\"\\nSample prediction:\")\n",
    "    print(f\"Text: {sample_text}\")\n",
    "    print(f\"Predicted class: {pred_class}\")\n",
    "    print(f\"Probabilities: {pred_probs}\")\n",
    "except Exception as e:\n",
    "    print(f\"Prediction error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d23c635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Validation Results:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07f9c8bb01f434cbaa07084d15f650e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37906971b2da406981cfc5ec3f246d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 0.3247\n",
      "eval_accuracy: 0.9491\n",
      "eval_f1: 0.9490\n",
      "eval_precision: 0.9491\n",
      "eval_recall: 0.9491\n",
      "eval_runtime: 7.3961\n",
      "eval_samples_per_second: 143.3190\n",
      "eval_steps_per_second: 4.5970\n",
      "epoch: 24.8387\n",
      "\n",
      "üîé Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9549    0.9402    0.9475       518\n",
      "           1     0.9436    0.9576    0.9505       542\n",
      "\n",
      "    accuracy                         0.9491      1060\n",
      "   macro avg     0.9493    0.9489    0.9490      1060\n",
      "weighted avg     0.9491    0.9491    0.9490      1060\n",
      "\n",
      "\n",
      "üìä Test Results:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb60cf60d3fe4f20b4bbe925058ba759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a67648fd9064f599276922cbab7150c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_eval_loss: 0.3210\n",
      "test_eval_accuracy: 0.9509\n",
      "test_eval_f1: 0.9509\n",
      "test_eval_precision: 0.9511\n",
      "test_eval_recall: 0.9509\n",
      "test_eval_runtime: 7.5675\n",
      "test_eval_samples_per_second: 140.0720\n",
      "test_eval_steps_per_second: 4.4930\n",
      "test_epoch: 24.8387\n",
      "\n",
      "üîé Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9587    0.9403    0.9494       519\n",
      "           1     0.9437    0.9612    0.9524       541\n",
      "\n",
      "    accuracy                         0.9509      1060\n",
      "   macro avg     0.9512    0.9507    0.9509      1060\n",
      "weighted avg     0.9511    0.9509    0.9509      1060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ‚úÖ Evaluate on validation set\n",
    "print(\"\\nüìä Validation Results:\")\n",
    "val_preds_output = trainer.predict(dataset_splits[\"validation\"])\n",
    "val_labels = val_preds_output.label_ids\n",
    "val_preds = val_preds_output.predictions.argmax(-1)\n",
    "\n",
    "val_metrics = trainer.evaluate()\n",
    "for key, value in val_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# ‚úÖ Classification report on validation\n",
    "print(\"\\nüîé Classification Report (Validation):\")\n",
    "print(classification_report(val_labels, val_preds, digits=4))\n",
    "\n",
    "# ‚úÖ Evaluate on test set\n",
    "print(\"\\nüìä Test Results:\")\n",
    "test_preds_output = trainer.predict(dataset_splits[\"test\"])\n",
    "test_labels = test_preds_output.label_ids\n",
    "test_preds = test_preds_output.predictions.argmax(-1)\n",
    "\n",
    "test_metrics = trainer.evaluate(dataset_splits[\"test\"])\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"test_{key}: {value:.4f}\")\n",
    "\n",
    "# ‚úÖ Classification report on test\n",
    "print(\"\\nüîé Classification Report (Test):\")\n",
    "print(classification_report(test_labels, test_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Get predictions from validation or test set\n",
    "predictions = trainer.predict(dataset_splits[\"test\"])\n",
    "y_true = predictions.label_ids\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load logs from Trainer\n",
    "log_history = trainer.state.log_history\n",
    "log_df = pd.DataFrame(log_history)\n",
    "\n",
    "# Filter only relevant columns\n",
    "train_logs = log_df[log_df[\"loss\"].notna()]\n",
    "eval_logs = log_df[log_df[\"eval_loss\"].notna()]\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(train_logs[\"step\"], train_logs[\"loss\"], label=\"Train Loss\")\n",
    "# plt.plot(eval_logs[\"step\"], eval_logs[\"eval_loss\"], label=\"Eval Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "if \"eval_accuracy\" in eval_logs.columns:\n",
    "    plt.plot(eval_logs[\"step\"], eval_logs[\"eval_accuracy\"], label=\"Eval Accuracy\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load logs from Trainer\n",
    "log_history = trainer.state.log_history\n",
    "log_df = pd.DataFrame(log_history)\n",
    "\n",
    "# Filter only relevant columns\n",
    "train_logs = log_df[log_df[\"loss\"].notna()]\n",
    "eval_logs = log_df[log_df[\"eval_loss\"].notna()]\n",
    "\n",
    "# Plot Loss\n",
    "# plt.plot(train_logs[\"step\"], train_logs[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(eval_logs[\"step\"], eval_logs[\"eval_loss\"], label=\"Eval Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "if \"eval_accuracy\" in eval_logs.columns:\n",
    "    plt.plot(eval_logs[\"step\"], eval_logs[\"eval_accuracy\"], label=\"Eval Accuracy\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Get softmax probabilities\n",
    "probs = predictions.predictions\n",
    "probs_softmax = torch.nn.functional.softmax(torch.tensor(probs), dim=1).numpy()\n",
    "\n",
    "# ROC Curve (for binary classification only)\n",
    "fpr, tpr, thresholds = roc_curve(y_true, probs_softmax[:, 1])\n",
    "auc_score = roc_auc_score(y_true, probs_softmax[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC-ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a541cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true, probs_softmax[:, 1])\n",
    "avg_precision = average_precision_score(y_true, probs_softmax[:, 1])\n",
    "\n",
    "plt.plot(recall, precision, label=f\"AP = {avg_precision:.4f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import numpy as np\n",
    "import datasets.formatting.formatting\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# ‚úÖ Monkey patch for NumPy 2.0 error in datasets\n",
    "def fixed_arrow_array_to_numpy(self, pa_array):\n",
    "    array = pa_array.to_pandas().values\n",
    "    return np.array(array, copy=True)\n",
    "\n",
    "datasets.formatting.formatting.NumpyArrowExtractor._arrow_array_to_numpy = fixed_arrow_array_to_numpy\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# ‚úÖ Disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ‚úÖ Load dataframe with better preprocessing\n",
    "df = pd.read_excel(r\"BanglaBlendCleanedDataWithEnglishTranslation.xlsx\")\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "# ‚úÖ Keep only needed columns and rename\n",
    "df = df[[\"Sentence\", \"Labels\"]].copy()\n",
    "df = df.rename(columns={\"Sentence\": \"text\", \"Labels\": \"label\"})\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# ‚úÖ Basic text preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# ‚úÖ Remove empty texts and duplicates\n",
    "df = df[df[\"text\"].str.len() > 0]\n",
    "df = df.drop_duplicates(subset=[\"text\"])\n",
    "\n",
    "print(\"Columns after clean & rename:\", df.columns)\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"Label distribution:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df.head())\n",
    "\n",
    "# ‚úÖ Load tokenizer and model with built-in regularization\n",
    "model_name = \"sagorsarker/bangla-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use standard model with built-in dropout configuration\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.5,  # Increase dropout for regularization\n",
    "    attention_probs_dropout_prob=0.3,  # Attention dropout\n",
    "    classifier_dropout=0.1  # Classifier dropout\n",
    ").to(device)\n",
    "\n",
    "# ‚úÖ Advanced tokenization with dynamic padding\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"], \n",
    "        padding=False,  # Dynamic padding in data collator\n",
    "        truncation=True, \n",
    "        max_length=256,  # Increased max length for better context\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# ‚úÖ Create dataset with stratified split\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# ‚úÖ Stratified train-validation-test split\n",
    "def create_stratified_splits(dataset, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    labels = [item['label'] for item in dataset]\n",
    "    \n",
    "    # First split: train and temp (val + test)\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=(val_size + test_size), random_state=random_state)\n",
    "    train_idx, temp_idx = next(sss1.split(range(len(labels)), labels))\n",
    "    \n",
    "    # Second split: val and test from temp\n",
    "    temp_labels = [labels[i] for i in temp_idx]\n",
    "    val_ratio = val_size / (val_size + test_size)\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=(1 - val_ratio), random_state=random_state)\n",
    "    val_idx_temp, test_idx_temp = next(sss2.split(range(len(temp_idx)), temp_labels))\n",
    "    \n",
    "    val_idx = [temp_idx[i] for i in val_idx_temp]\n",
    "    test_idx = [temp_idx[i] for i in test_idx_temp]\n",
    "    \n",
    "    return {\n",
    "        'train': dataset.select(train_idx),\n",
    "        'validation': dataset.select(val_idx),\n",
    "        'test': dataset.select(test_idx)\n",
    "    }\n",
    "\n",
    "dataset_splits = create_stratified_splits(dataset)\n",
    "\n",
    "# ‚úÖ Set torch format\n",
    "for split in dataset_splits:\n",
    "    dataset_splits[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "print(f\"Train size: {len(dataset_splits['train'])}\")\n",
    "print(f\"Validation size: {len(dataset_splits['validation'])}\")\n",
    "print(f\"Test size: {len(dataset_splits['test'])}\")\n",
    "\n",
    "# ‚úÖ Data collator for dynamic padding\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ‚úÖ Enhanced metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    recall = recall_score(labels, preds, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ Enhanced training arguments with regularization\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=20,  # Reduced epochs to prevent overfitting\n",
    "    per_device_train_batch_size=16,  # Increased batch size\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,  # Effective batch size = 32\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # ‚úÖ Learning rate scheduling\n",
    "    learning_rate=3e-5,  # Slightly higher learning rate\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=500,\n",
    "    \n",
    "    # ‚úÖ Regularization techniques\n",
    "    weight_decay=0.1,  # L2 regularization\n",
    "    adam_epsilon=1e-6,\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    label_smoothing_factor=0.1,  # Built-in label smoothing\n",
    "    \n",
    "    # ‚úÖ Early stopping and model selection\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # ‚úÖ Evaluation and saving\n",
    "    eval_steps=100,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # ‚úÖ Other optimizations\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# ‚úÖ Custom Trainer to track training history\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.epochs = []\n",
    "    \n",
    "    def log(self, logs):\n",
    "        super().log(logs)\n",
    "        # Track training loss\n",
    "        if \"train_loss\" in logs:\n",
    "            self.train_losses.append(logs[\"train_loss\"])\n",
    "        \n",
    "        # Track validation metrics\n",
    "        if \"eval_loss\" in logs:\n",
    "            self.val_losses.append(logs[\"eval_loss\"])\n",
    "            self.val_accuracies.append(logs.get(\"eval_accuracy\", 0))\n",
    "            self.epochs.append(len(self.val_losses))\n",
    "\n",
    "# ‚úÖ Standard trainer with built-in label smoothing\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_splits[\"train\"],\n",
    "    eval_dataset=dataset_splits[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# ‚úÖ Train with mixed precision for efficiency\n",
    "print(\"üöÄ Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# ‚úÖ Evaluate on validation set\n",
    "print(\"\\nüìä Validation Results:\")\n",
    "val_metrics = trainer.evaluate()\n",
    "for key, value in val_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# ‚úÖ Evaluate on test set\n",
    "print(\"\\nüìä Test Results:\")\n",
    "test_metrics = trainer.evaluate(dataset_splits[\"test\"])\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"test_{key}: {value:.4f}\")\n",
    "\n",
    "# ‚úÖ Calculate overfitting metrics\n",
    "train_metrics = trainer.evaluate(dataset_splits[\"train\"])\n",
    "overfitting_accuracy = train_metrics['eval_accuracy'] - test_metrics['eval_accuracy']\n",
    "overfitting_f1 = train_metrics['eval_f1'] - test_metrics['eval_f1']\n",
    "\n",
    "print(f\"\\nüéØ Overfitting Analysis:\")\n",
    "print(f\"Training Accuracy: {train_metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"Accuracy Gap (overfitting): {overfitting_accuracy:.4f}\")\n",
    "print(f\"F1 Gap (overfitting): {overfitting_f1:.4f}\")\n",
    "\n",
    "# ‚úÖ Save the best model\n",
    "model.save_pretrained(\"./final_bangla_bert_model\")\n",
    "tokenizer.save_pretrained(\"./final_bangla_bert_model\")\n",
    "print(\"\\n‚úÖ Best model saved at ./final_bangla_bert_model\")\n",
    "\n",
    "# ‚úÖ Generate predictions for confusion matrix\n",
    "def get_predictions(trainer, dataset):\n",
    "    \"\"\"Get predictions from the model\"\"\"\n",
    "    predictions = trainer.predict(dataset)\n",
    "    y_pred = predictions.predictions.argmax(-1)\n",
    "    y_true = predictions.label_ids\n",
    "    return y_true, y_pred\n",
    "\n",
    "# ‚úÖ Create confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=None, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# ‚úÖ Plot training history\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"Plot training and validation loss/accuracy over epochs\"\"\"\n",
    "    epochs = range(1, len(trainer.val_losses) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(epochs, trainer.val_losses, 'b-', label='Validation Loss', linewidth=2)\n",
    "    if trainer.train_losses:\n",
    "        # Match training losses to validation epochs\n",
    "        train_epochs = np.linspace(1, len(trainer.val_losses), len(trainer.train_losses))\n",
    "        ax1.plot(train_epochs, trainer.train_losses, 'r-', label='Training Loss', linewidth=2)\n",
    "    \n",
    "    ax1.set_title('Training & Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    ax2.plot(epochs, trainer.val_accuracies, 'g-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ‚úÖ Generate confusion matrices for validation and test sets\n",
    "print(\"\\nüìä Generating Confusion Matrices...\")\n",
    "\n",
    "# Get unique class names\n",
    "class_names = [f\"Class {i}\" for i in sorted(df['label'].unique())]\n",
    "\n",
    "# Validation set confusion matrix\n",
    "val_true, val_pred = get_predictions(trainer, dataset_splits[\"validation\"])\n",
    "print(f\"\\nValidation Confusion Matrix:\")\n",
    "val_cm = plot_confusion_matrix(val_true, val_pred, class_names, \"Validation Set Confusion Matrix\")\n",
    "print(val_cm)\n",
    "\n",
    "# Test set confusion matrix\n",
    "test_true, test_pred = get_predictions(trainer, dataset_splits[\"test\"])\n",
    "print(f\"\\nTest Confusion Matrix:\")\n",
    "test_cm = plot_confusion_matrix(test_true, test_pred, class_names, \"Test Set Confusion Matrix\")\n",
    "print(test_cm)\n",
    "\n",
    "# ‚úÖ Plot training history\n",
    "print(\"\\nüìà Training History:\")\n",
    "plot_training_history(trainer)\n",
    "\n",
    "# ‚úÖ Additional classification metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\nüìä Detailed Classification Report (Test Set):\")\n",
    "print(classification_report(test_true, test_pred, target_names=class_names))\n",
    "\n",
    "print(\"\\nüìä Detailed Classification Report (Validation Set):\")\n",
    "print(classification_report(val_true, val_pred, target_names=class_names))\n",
    "\n",
    "# ‚úÖ Optional: Model inference example\n",
    "def predict_text(text, model, tokenizer, device):\n",
    "    \"\"\"Function to predict on new text\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=256\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1)\n",
    "    \n",
    "    return predicted_class.item(), predictions.cpu().numpy()\n",
    "\n",
    "# ‚úÖ Example usage\n",
    "sample_text = \"‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡•§\"\n",
    "try:\n",
    "    pred_class, pred_probs = predict_text(sample_text, model, tokenizer, device)\n",
    "    print(f\"\\nSample prediction:\")\n",
    "    print(f\"Text: {sample_text}\")\n",
    "    print(f\"Predicted class: {pred_class}\")\n",
    "    print(f\"Probabilities: {pred_probs}\")\n",
    "except Exception as e:\n",
    "    print(f\"Prediction error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
